> python 3.7
> node 14.15.0
> MySql 8.0+


首先是创建一个数据库

```sql
CREATE DATABASE TEST default character set utf8mb4 collate utf8mb4_unicode_ci;
```

然后是安装依赖

```shell
pip install -r requirements.txt
```

## 前端编译：

注意，需要先去 `webview`中运行如下命令：

1. npm install
2. npm run build

然后把生成的 `dist` 目录下的 `index.html` 拷贝到 `templates` 目录下，把 `webview/dist/static` 目录下的所有拷贝到 `webview/static` 目录下

---

## 下面是启动爬虫

1. 修改 `scrapydserver/build/lib/bot/spiders/weibo_spider.py` 里面的数据库连接
2. 修改 `scrapydserver/bot/spiders/weibo_spider.py` 里面的数据库连接
3. 修改 `scrapydserver/bot/settings.py` 里面的数据库连接
3. scrapyd 命令启动爬虫

---

## 下面是Django的启动方式:

1. 修改mysql数据库的账户密码（weibosystem/ssettings）里面

2. 同步数据库 

```shell
python manage.py makemigrations 
python manage.py migrate
```

3. 创建后台xadmin账户python manage.py createsuperuser

4. 启动Django服务

```shell
python manage.py runserver
```

5. 登录后台，http://localhost:8000/xadmin/SpiderAPI/target/ 在爬虫API里面的爬虫设置，输入一个用户uid + cookie，然后即可开始在首页localhost:8000数据爬虫id爬虫

---

TIPS:

注意：怎么获取微博的登录Cookie，请登录后，请求这个网站 [https://weibo.cn/1767123691/info](https://weibo.cn/1767123691/info)，然后把cookie填入

---

怎么获取 单条微博id，可以给微博点赞，然后看 开发者模式里面的 Network，筛选 `weibo.com/aj/v6/like/add` 这个，然后就能看到请求参数中有个 mid: 4850671050040090，这就是单条微博ID

